"""Tests for persistence.py to increase coverage to 97%+."""
# pyright: reportArgumentType=false, reportOptionalSubscript=false, reportOperatorIssue=false, reportAttributeAccessIssue=false, reportOptionalMemberAccess=false, reportOptionalCall=false, reportRedeclaration=false, reportIncompatibleMethodOverride=false, reportGeneralTypeIssues=false, reportSelfClsParameterName=false, reportCallIssue=false, reportOptionalIterable=false

from __future__ import annotations

import asyncio
import contextlib
import datetime
import json
import sys
import types
from concurrent.futures import ThreadPoolExecutor
from typing import Any, cast
from unittest.mock import MagicMock, Mock, patch

import pytest
from sqlalchemy import create_engine
from sqlalchemy.exc import SQLAlchemyError

from open_webui_openrouter_pipe import Pipe
from open_webui_openrouter_pipe.storage import persistence as persistence_mod
from open_webui_openrouter_pipe.storage.persistence import (
    ArtifactStore,
    generate_item_id,
    normalize_persisted_item,
    _normalize_persisted_item,
    _wait_for,
    _encode_crockford,
    _sanitize_table_fragment,
)


# -----------------------------------------------------------------------------
# Fixtures and Helpers
# -----------------------------------------------------------------------------


class _Field:
    def __init__(self, name: str) -> None:
        self.name = name

    def __eq__(self, other):
        return ("eq", self.name, other)

    def __lt__(self, other):
        return ("lt", self.name, other)

    def in_(self, values):
        return ("in", self.name, list(values))

    def asc(self):
        return ("asc", self.name)

    def desc(self):
        return ("desc", self.name)


class _FakeModel:
    id = _Field("id")
    chat_id = _Field("chat_id")
    message_id = _Field("message_id")
    model_id = _Field("model_id")
    item_type = _Field("item_type")
    payload = _Field("payload")
    is_encrypted = _Field("is_encrypted")
    created_at = _Field("created_at")

    def __init__(self, **kwargs: Any) -> None:
        for key, value in kwargs.items():
            setattr(self, key, value)


class _FakeQuery:
    def __init__(self, rows: list[_FakeModel], select_fields: list[_Field] | None = None) -> None:
        self._rows = rows
        self._filters: list[tuple[str, str, Any]] = []
        self._order: tuple[str, str] | None = None
        self._limit: int | None = None
        self._select_fields = select_fields

    def filter(self, condition):
        self._filters.append(condition)
        return self

    def order_by(self, order):
        if isinstance(order, tuple):
            self._order = (order[0], order[1])
        return self

    def limit(self, limit: int):
        self._limit = int(limit)
        return self

    def distinct(self):
        return self

    def _match(self, row: _FakeModel, condition: tuple[str, str, Any]) -> bool:
        op, name, value = condition
        current = getattr(row, name, None)
        if op == "eq":
            return current == value
        if op == "in":
            return current in value
        if op == "lt":
            return current < value
        return False

    def _apply(self) -> list[_FakeModel]:
        results = [row for row in self._rows if all(self._match(row, cond) for cond in self._filters)]
        if self._order:
            direction, name = self._order
            reverse = direction == "desc"
            results.sort(key=lambda row: cast(Any, getattr(row, name, None)), reverse=reverse)
        if self._limit is not None:
            results = results[: self._limit]
        return results

    def all(self):
        rows = self._apply()
        if not self._select_fields:
            return rows
        if len(self._select_fields) == 1:
            field = self._select_fields[0].name
            return [(getattr(row, field, None),) for row in rows]
        return [tuple(getattr(row, field.name, None) for field in self._select_fields) for row in rows]

    def first(self):
        rows = self.all()
        return rows[0] if rows else None

    def update(self, values, synchronize_session: bool = False):
        rows = self._apply()
        for row in rows:
            for field, val in values.items():
                if isinstance(field, _Field):
                    setattr(row, field.name, val)
        return len(rows)

    def delete(self, synchronize_session: bool = False):
        rows = self._apply()
        for row in rows:
            self._rows.remove(row)
        return len(rows)


class _FakeSession:
    def __init__(self, rows: list[_FakeModel], fail_on_commit: bool = False) -> None:
        self._rows = rows
        self._fail_on_commit = fail_on_commit

    def add_all(self, instances: list[_FakeModel]) -> None:
        self._rows.extend(instances)

    def commit(self) -> None:
        if self._fail_on_commit:
            raise SQLAlchemyError("commit failed")
        return None

    def rollback(self) -> None:
        return None

    def close(self) -> None:
        return None

    def query(self, *fields):
        if not fields:
            return _FakeQuery(self._rows)
        if len(fields) == 1 and fields[0] is _FakeModel:
            return _FakeQuery(self._rows)
        select_fields = [field for field in fields if isinstance(field, _Field)]
        return _FakeQuery(self._rows, select_fields=select_fields)


@contextlib.contextmanager
def _install_internal_db(engine, schema: str | None = None):
    saved_internal = sys.modules.get("open_webui.internal")
    saved_db = sys.modules.get("open_webui.internal.db")

    internal_pkg = types.ModuleType("open_webui.internal")
    db_mod = types.ModuleType("open_webui.internal.db")
    setattr(db_mod, "engine", engine)
    setattr(db_mod, "ENGINE", engine)
    if schema is not None:
        class _Base:
            metadata = types.SimpleNamespace(schema=schema)
        setattr(db_mod, "Base", _Base)

    setattr(internal_pkg, "db", db_mod)
    sys.modules["open_webui.internal"] = internal_pkg
    sys.modules["open_webui.internal.db"] = db_mod
    try:
        yield
    finally:
        if saved_internal is not None:
            sys.modules["open_webui.internal"] = saved_internal
        else:
            sys.modules.pop("open_webui.internal", None)
        if saved_db is not None:
            sys.modules["open_webui.internal.db"] = saved_db
        else:
            sys.modules.pop("open_webui.internal.db", None)


def _install_fake_store(pipe: Pipe, fail_on_commit: bool = False) -> list[_FakeModel]:
    rows: list[_FakeModel] = []
    store = pipe._artifact_store
    store_any = cast(Any, store)
    store_any._item_model = _FakeModel
    store_any._session_factory = lambda: _FakeSession(rows, fail_on_commit)
    store_any._artifact_table_name = "response_items_test"
    store_any._db_executor = ThreadPoolExecutor(max_workers=1)
    return rows


def _make_row(chat_id: str, message_id: str, payload: dict[str, Any]) -> dict[str, Any]:
    return {
        "chat_id": chat_id,
        "message_id": message_id,
        "model_id": "model",
        "item_type": payload.get("type", "unknown"),
        "payload": payload,
    }


def _sqlite_engine():
    return create_engine("sqlite://")


# -----------------------------------------------------------------------------
# Tests for Schema Discovery Edge Cases (lines 351-352, 363-364, 393-394, 407)
# -----------------------------------------------------------------------------


def test_discover_schema_from_base_metadata_raises(pipe_instance):
    """Test exception handling in Base.metadata.schema discovery (lines 351-352)."""
    class _DB:
        class Base:
            @property
            def metadata(self):
                raise RuntimeError("metadata access failed")

    engine, schema, details = pipe_instance._discover_owui_engine_and_schema(_DB)
    assert schema is None


def test_discover_schema_from_metadata_obj_raises(pipe_instance):
    """Test exception handling in metadata_obj.schema discovery (lines 363-364)."""
    class _DB:
        Base = None

        @property
        def metadata_obj(self):
            raise RuntimeError("metadata_obj access failed")

    engine, schema, details = pipe_instance._discover_owui_engine_and_schema(_DB)
    assert schema is None


def test_discover_engine_from_bind_fallback(pipe_instance):
    """Test engine discovery via session.bind when get_bind fails (lines 389-394)."""
    engine_obj = object()

    class _Session:
        def __init__(self):
            self.bind = engine_obj

        def get_bind(self):
            raise RuntimeError("get_bind failed")

        def close(self):
            pass

    @contextlib.contextmanager
    def _ctx():
        yield _Session()

    class _DB:
        Base = None
        get_db_context = staticmethod(_ctx)

    discovered_engine, schema, details = pipe_instance._discover_owui_engine_and_schema(_DB)
    assert discovered_engine is engine_obj
    assert details.get("engine_source") == "get_db_context.bind"


def test_discover_engine_unavailable_sets_details(pipe_instance):
    """Test that engine_source is set to 'unavailable' when no engine found (line 407)."""
    class _DB:
        pass

    engine, schema, details = pipe_instance._discover_owui_engine_and_schema(_DB)
    assert engine is None
    assert details.get("engine_source") == "unavailable"
    assert details.get("schema_source") == "unavailable"


def test_discover_engine_from_context_raises(pipe_instance):
    """Test exception handling in get_db_context (lines 393-394)."""
    @contextlib.contextmanager
    def _ctx():
        raise RuntimeError("context failed")
        yield

    class _DB:
        Base = None
        get_db_context = staticmethod(_ctx)

    engine, schema, details = pipe_instance._discover_owui_engine_and_schema(_DB)
    assert engine is None


# -----------------------------------------------------------------------------
# Tests for Dialect Logging Exception (lines 440-441)
# -----------------------------------------------------------------------------


def test_init_artifact_store_dialect_logging_exception(pipe_instance, caplog):
    """Test that dialect logging exception is handled gracefully (lines 440-441)."""
    class _BrokenDialect:
        @property
        def name(self):
            raise RuntimeError("dialect name failed")

    class _BrokenEngine:
        @property
        def dialect(self):
            return _BrokenDialect()

    with _install_internal_db(_BrokenEngine()):
        # The exception in dialect logging should be caught and alternative logging used
        pipe_instance._init_artifact_store(pipe_identifier="test_dialect", table_fragment="test")


# -----------------------------------------------------------------------------
# Tests for Schema Normalization (lines 477, 481-483, 490)
# -----------------------------------------------------------------------------


def test_schema_normalization_logic():
    """Test schema normalization logic (lines 480-483, 489-490).

    This directly tests the schema normalization logic that determines whether
    to add schema to table_args.
    """
    # Test case 1: Valid schema with whitespace
    schema = "  test_schema  "
    normalized_schema = None
    if isinstance(schema, str):
        candidate = schema.strip()
        if candidate:
            normalized_schema = candidate
    assert normalized_schema == "test_schema"

    # Test case 2: Empty string after strip
    schema = "   "
    normalized_schema = None
    if isinstance(schema, str):
        candidate = schema.strip()
        if candidate:
            normalized_schema = candidate
    assert normalized_schema is None

    # Test case 3: None schema
    schema = None
    normalized_schema = None
    if isinstance(schema, str):
        candidate = schema.strip()
        if candidate:
            normalized_schema = candidate
    assert normalized_schema is None

    # Test case 4: table_args with schema
    table_args = {"extend_existing": True, "sqlite_autoincrement": False}
    normalized_schema = "my_schema"
    if normalized_schema:
        table_args["schema"] = normalized_schema
    assert table_args["schema"] == "my_schema"

    # Test case 5: table_args without schema
    table_args = {"extend_existing": True, "sqlite_autoincrement": False}
    normalized_schema = None
    if normalized_schema:
        table_args["schema"] = normalized_schema
    assert "schema" not in table_args


def test_init_artifact_store_existing_table_removal(pipe_instance):
    """Test that existing table metadata is removed (line 477)."""
    engine = _sqlite_engine()
    with _install_internal_db(engine):
        # Initialize once
        pipe_instance._init_artifact_store(pipe_identifier="pipe", table_fragment="pipe")
        # Initialize again with same table name to trigger removal
        pipe_instance._artifact_store._artifact_store_signature = None
        pipe_instance._init_artifact_store(pipe_identifier="pipe", table_fragment="pipe")

    store = pipe_instance._artifact_store
    assert store._item_model is not None


# -----------------------------------------------------------------------------
# Tests for Table Inspection Exception (lines 515-516)
# -----------------------------------------------------------------------------


def test_init_artifact_store_inspection_error(pipe_instance, monkeypatch):
    """Test SQLAlchemyError during table inspection sets table_exists=True (lines 515-516)."""
    engine = _sqlite_engine()

    def _raise_inspection(*args, **kwargs):
        raise SQLAlchemyError("inspection failed")

    with _install_internal_db(engine):
        monkeypatch.setattr(persistence_mod, "sa_inspect", _raise_inspection)
        pipe_instance._init_artifact_store(pipe_identifier="pipe_inspect", table_fragment="pipe_inspect")

    store = pipe_instance._artifact_store
    assert store._item_model is not None


# -----------------------------------------------------------------------------
# Tests for Index Healing Edge Cases (lines 607, 613, 635)
# -----------------------------------------------------------------------------


def test_maybe_heal_index_conflict_no_indexes(pipe_instance):
    """Test _maybe_heal_index_conflict returns False when no indexes to drop (line 607)."""
    store = pipe_instance._artifact_store

    class _DummyTable:
        name = "test_table"
        schema = None
        indexes = set()
        columns = []

    result = store._maybe_heal_index_conflict(
        Mock(),
        _DummyTable(),
        Exception("some error without ix_"),
    )
    assert result is False


def test_maybe_heal_index_conflict_empty_name(pipe_instance):
    """Test _maybe_heal_index_conflict skips empty index names (line 613)."""
    store = pipe_instance._artifact_store

    class _DummyIndex:
        name = ""

    class _DummyTable:
        name = "test_table"
        schema = None
        indexes = {_DummyIndex()}
        columns = []

    executed = []

    class _DummyEngine:
        def begin(self):
            class _Conn:
                def execute(self, stmt):
                    executed.append(str(stmt))
            return contextlib.nullcontext(_Conn())

    result = store._maybe_heal_index_conflict(
        _DummyEngine(),
        _DummyTable(),
        Exception("error ix_test_table_something"),
    )
    # Should return True if it found and dropped something, or False if nothing dropped
    # With empty name, nothing should be executed from metadata indexes
    assert isinstance(result, bool)


def test_maybe_heal_index_conflict_all_fail(pipe_instance):
    """Test _maybe_heal_index_conflict returns False when all drops fail (line 635)."""
    store = pipe_instance._artifact_store

    class _DummyIndex:
        name = "ix_test_table_col"

    class _DummyTable:
        name = "test_table"
        schema = None
        indexes = {_DummyIndex()}
        columns = []

    class _DummyEngine:
        def begin(self):
            class _Conn:
                def execute(self, stmt):
                    raise SQLAlchemyError("drop failed")
            return contextlib.nullcontext(_Conn())

    result = store._maybe_heal_index_conflict(
        _DummyEngine(),
        _DummyTable(),
        Exception("error ix_test_table_col"),
    )
    assert result is False


# -----------------------------------------------------------------------------
# Tests for Prepare Rows Edge Cases (line 778)
# -----------------------------------------------------------------------------


def test_prepare_rows_for_storage_non_dict_payload(pipe_instance):
    """Test _prepare_rows_for_storage skips non-dict payloads (line 778)."""
    store = pipe_instance._artifact_store
    rows = [
        {"payload": "not a dict", "item_type": "note"},
        {"payload": 123, "item_type": "note"},
        {"payload": None, "item_type": "note"},
    ]
    store._prepare_rows_for_storage(rows)
    # Should not crash, payloads remain unchanged
    assert rows[0]["payload"] == "not a dict"
    assert rows[1]["payload"] == 123


# -----------------------------------------------------------------------------
# Tests for DB Persist Edge Cases (lines 812, 916, 975, 980)
# -----------------------------------------------------------------------------


def test_db_persist_sync_empty_rows(pipe_instance):
    """Test _db_persist_sync returns empty list for empty rows (line 812)."""
    _install_fake_store(pipe_instance)
    result = pipe_instance._db_persist_sync([])
    assert result == []


@pytest.mark.asyncio
async def test_db_persist_empty_rows(pipe_instance):
    """Test _db_persist returns empty list for empty rows (line 916)."""
    _install_fake_store(pipe_instance)
    result = await pipe_instance._db_persist([])
    assert result == []


@pytest.mark.asyncio
async def test_db_persist_direct_reraises_non_duplicate(pipe_instance, monkeypatch):
    """Test _db_persist_direct re-raises non-duplicate key errors (line 975)."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store
    store._redis_enabled = False

    call_count = [0]

    def _boom(_rows):
        call_count[0] += 1
        raise SQLAlchemyError("other error not duplicate")

    monkeypatch.setattr(store, "_db_persist_sync", _boom)

    with pytest.raises(SQLAlchemyError):
        await store._db_persist_direct([{"id": "id-1", "chat_id": "c", "message_id": "m"}])


# -----------------------------------------------------------------------------
# Tests for Duplicate Key Detection (lines 988, 992)
# -----------------------------------------------------------------------------


def test_is_duplicate_key_error_with_orig(pipe_instance):
    """Test _is_duplicate_key_error checks orig attribute (line 988)."""
    store = pipe_instance._artifact_store

    class _OrigError:
        def __str__(self):
            return "duplicate key value"

    exc = SQLAlchemyError("parent error")
    exc.orig = _OrigError()
    assert store._is_duplicate_key_error(exc) is True


def test_is_duplicate_key_error_non_sqlalchemy(pipe_instance):
    """Test _is_duplicate_key_error returns False for non-SQLAlchemy errors (line 992)."""
    store = pipe_instance._artifact_store
    assert store._is_duplicate_key_error(RuntimeError("duplicate key")) is False
    assert store._is_duplicate_key_error(ValueError("unique constraint")) is False


# -----------------------------------------------------------------------------
# Tests for DB Fetch Edge Cases (lines 1003, 1049-1050, 1085, 1095, 1098, 1132, 1157)
# -----------------------------------------------------------------------------


def test_db_fetch_sync_no_item_ids(pipe_instance):
    """Test _db_fetch_sync returns empty dict for empty item_ids (line 1003)."""
    _install_fake_store(pipe_instance)
    result = pipe_instance._db_fetch_sync("chat", "msg", [])
    assert result == {}


def test_db_fetch_sync_no_session_factory(pipe_instance):
    """Test _db_fetch_sync returns empty dict when no session_factory (line 1003)."""
    store = pipe_instance._artifact_store
    store._session_factory = None
    result = store._db_fetch_sync("chat", "msg", ["id-1"])
    assert result == {}


def test_db_fetch_sync_touch_outer_exception(pipe_instance):
    """Test _db_fetch_sync handles outer touch exception (lines 1049-1050)."""
    rows: list[_FakeModel] = []
    store = pipe_instance._artifact_store

    class _FailingSession:
        def __init__(self, rows):
            self._rows = rows
            self._call_count = 0

        def query(self, *args):
            self._call_count += 1
            if self._call_count == 1:
                return _FakeQuery(self._rows)
            # Second call (touch session) - raise before query
            raise RuntimeError("outer touch failed")

        def close(self):
            pass

    call_count = [0]
    def _session_factory():
        call_count[0] += 1
        if call_count[0] == 1:
            # First session for fetch
            return _FakeSession(rows)
        # Second session for touch - will fail
        raise RuntimeError("touch session creation failed")

    store._item_model = _FakeModel
    store._session_factory = _session_factory
    store._artifact_table_name = "test"
    store._db_executor = ThreadPoolExecutor(max_workers=1)

    # Add a row to be fetched
    row = _FakeModel(
        id="id-1",
        chat_id="chat",
        message_id="msg",
        payload={"type": "note"},
        is_encrypted=False,
    )
    rows.append(row)

    # Should not crash, just log debug message
    result = store._db_fetch_sync("chat", "msg", ["id-1"])
    assert "id-1" in result


@pytest.mark.asyncio
async def test_db_fetch_empty_chat_id(pipe_instance):
    """Test _db_fetch returns empty dict for empty chat_id (line 1085)."""
    _install_fake_store(pipe_instance)
    result = await pipe_instance._db_fetch("", "msg", ["id-1"])
    assert result == {}

    result = await pipe_instance._db_fetch(None, "msg", ["id-1"])
    assert result == {}


@pytest.mark.asyncio
async def test_db_fetch_no_missing_ids(pipe_instance):
    """Test _db_fetch returns cached when no missing IDs (line 1095)."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store
    store._redis_enabled = True

    async def _fake_redis_fetch(_chat_id, item_ids):
        return {item_id: {"type": "cached"} for item_id in item_ids}

    store._redis_fetch_rows = _fake_redis_fetch

    result = await store._db_fetch("chat", "msg", ["id-1"])
    assert result == {"id-1": {"type": "cached"}}


@pytest.mark.asyncio
async def test_db_fetch_no_db_executor(pipe_instance):
    """Test _db_fetch returns cached when no db_executor (line 1098)."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store
    store._db_executor = None

    result = await store._db_fetch("chat", "msg", ["id-1"])
    assert result == {}


@pytest.mark.asyncio
async def test_db_fetch_resets_failure_on_success(pipe_instance, monkeypatch):
    """Test _db_fetch resets failure count on success (line 1132)."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store

    reset_calls = []
    monkeypatch.setattr(store, "_reset_db_failure", lambda uid: reset_calls.append(uid))

    async def _fake_fetch_direct(*args):
        return {"id-1": {"type": "note"}}

    monkeypatch.setattr(store, "_db_fetch_direct", _fake_fetch_direct)

    # Need to set a user_id context
    from open_webui_openrouter_pipe.core.logging_system import SessionLogger
    token = SessionLogger.user_id.set("test-user")
    try:
        result = await store._db_fetch("chat", "msg", ["id-1"])
        assert "test-user" in reset_calls
    finally:
        SessionLogger.user_id.reset(token)


# -----------------------------------------------------------------------------
# Tests for Redis Pubsub Edge Cases (lines 1202, 1208, 1212-1213)
# -----------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_redis_pubsub_listener_no_client(pipe_instance):
    """Test _redis_pubsub_listener returns early when no client (line 1202)."""
    store = pipe_instance._artifact_store
    store._redis_client = None
    await store._redis_pubsub_listener()


@pytest.mark.asyncio
async def test_redis_pubsub_listener_skips_non_message(pipe_instance):
    """Test _redis_pubsub_listener skips non-message types (line 1208)."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store

    flush_calls = []

    class _PubSub:
        async def subscribe(self, _channel):
            pass

        async def listen(self):
            yield {"type": "subscribe", "data": None}
            yield {"type": "psubscribe", "data": None}
            # Only this one should trigger flush
            yield {"type": "message", "data": "flush"}

        async def close(self):
            pass

    class _FakeRedis:
        def pubsub(self):
            return _PubSub()

    async def _fake_flush():
        flush_calls.append("flushed")

    store._redis_client = _FakeRedis()
    store._flush_redis_queue = _fake_flush

    await store._redis_pubsub_listener()
    assert flush_calls == ["flushed"]


@pytest.mark.asyncio
async def test_redis_pubsub_listener_exception(pipe_instance, caplog):
    """Test _redis_pubsub_listener handles exception (lines 1212-1213)."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store

    class _PubSub:
        async def subscribe(self, _channel):
            pass

        async def listen(self):
            raise RuntimeError("pubsub failed")
            yield

        async def close(self):
            pass

    class _FakeRedis:
        def pubsub(self):
            return _PubSub()

    store._redis_client = _FakeRedis()

    caplog.set_level("WARNING")
    await store._redis_pubsub_listener()
    assert any("pub/sub listener stopped" in rec.message for rec in caplog.records)


# -----------------------------------------------------------------------------
# Tests for Redis Periodic Flusher Edge Cases (lines 1227, 1234-1235, 1255)
# -----------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_redis_periodic_flusher_no_client(pipe_instance, monkeypatch):
    """Test _redis_periodic_flusher exits when no redis client (line 1227)."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store
    store._redis_enabled = True
    store._redis_client = None

    await store._redis_periodic_flusher()
    # Should exit immediately


@pytest.mark.asyncio
async def test_redis_periodic_flusher_queue_depth_logging(pipe_instance, monkeypatch, caplog):
    """Test _redis_periodic_flusher logs queue depth (lines 1234-1235)."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store
    store._redis_enabled = True
    store.valves.REDIS_PENDING_WARN_THRESHOLD = 100

    class _FakeRedis:
        def __init__(self):
            self.llen_calls = 0

        def llen(self, _key):
            self.llen_calls += 1
            if self.llen_calls == 1:
                return 5  # Below warn threshold but > 0
            return 0

    store._redis_client = _FakeRedis()

    async def _fake_flush():
        pass

    async def _fake_sleep(_seconds):
        store._redis_enabled = False

    store._flush_redis_queue = _fake_flush
    monkeypatch.setattr(asyncio, "sleep", _fake_sleep)

    caplog.set_level("DEBUG")
    await store._redis_periodic_flusher()
    assert any("queue depth" in rec.message for rec in caplog.records)


@pytest.mark.asyncio
async def test_flush_redis_queue_disabled(pipe_instance):
    """Test _flush_redis_queue returns early when disabled (line 1255)."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store
    store._redis_enabled = False
    store._redis_client = Mock()

    await store._flush_redis_queue()
    # Should return without doing anything


# -----------------------------------------------------------------------------
# Tests for Redis Lock Release Edge Cases (lines 1337-1338)
# -----------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_flush_redis_queue_lock_release_not_1(pipe_instance, caplog):
    """Test _flush_redis_queue warns when lock release returns non-1 (lines 1337-1338)."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store
    store._redis_enabled = True

    class _FakeRedis:
        def __init__(self):
            self.storage = {}
            self.lists = {}

        def set(self, key, value, nx=False, ex=None):
            if nx and key in self.storage:
                return False
            self.storage[key] = value
            return True

        def lpop(self, _key):
            return None

        def eval(self, _script, _numkeys, key, token):
            # Return 0 to indicate lock wasn't released properly
            return 0

    store._redis_client = _FakeRedis()

    caplog.set_level("WARNING")
    await store._flush_redis_queue()
    assert any("lock was not released" in rec.message for rec in caplog.records)


@pytest.mark.asyncio
async def test_flush_redis_queue_lock_release_non_int(pipe_instance, caplog):
    """Test _flush_redis_queue handles non-int lock release result (lines 1337-1338)."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store
    store._redis_enabled = True

    class _FakeRedis:
        def __init__(self):
            self.storage = {}
            self.lists = {}

        def set(self, key, value, nx=False, ex=None):
            self.storage[key] = value
            return True

        def lpop(self, _key):
            return None

        def eval(self, _script, _numkeys, key, token):
            return "not an int"

    store._redis_client = _FakeRedis()

    caplog.set_level("WARNING")
    await store._flush_redis_queue()
    assert any("lock was not released" in rec.message for rec in caplog.records)


# -----------------------------------------------------------------------------
# Tests for Redis Enqueue Edge Cases (lines 1359, 1362)
# -----------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_redis_enqueue_rows_empty(pipe_instance):
    """Test _redis_enqueue_rows returns empty list for empty rows (line 1359)."""
    _install_fake_store(pipe_instance)
    result = await pipe_instance._redis_enqueue_rows([])
    assert result == []


@pytest.mark.asyncio
async def test_redis_enqueue_rows_fallback_disabled(pipe_instance, monkeypatch):
    """Test _redis_enqueue_rows falls back to direct DB when redis disabled (line 1362)."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store
    store._redis_enabled = False
    store._redis_client = None

    direct_calls = []

    async def _fake_direct(rows, **kwargs):
        direct_calls.append(rows)
        return [row.get("id", generate_item_id()) for row in rows]

    monkeypatch.setattr(store, "_db_persist_direct", _fake_direct)

    rows = [_make_row("chat", "msg", {"type": "note"})]
    result = await store._redis_enqueue_rows(rows)
    assert len(result) == 1
    assert len(direct_calls) == 1


# -----------------------------------------------------------------------------
# Tests for Redis Fetch Edge Cases (lines 1423, 1443-1444)
# -----------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_redis_fetch_rows_empty_keys(pipe_instance):
    """Test _redis_fetch_rows returns empty dict when no keys (line 1423)."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store
    store._redis_enabled = True

    class _FakeRedis:
        pass

    store._redis_client = _FakeRedis()

    # Empty item_ids should result in empty keys
    result = await store._redis_fetch_rows("chat", [])
    assert result == {}


@pytest.mark.asyncio
async def test_redis_fetch_rows_ciphertext_from_nested_payload(pipe_instance):
    """Test _redis_fetch_rows extracts ciphertext from row_data.payload (lines 1443-1444)."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store
    store._redis_enabled = True
    store._encryption_key = "test-key-for-encryption"

    # First create encrypted data
    test_payload = {"type": "reasoning", "text": "secret"}
    encrypted = store._encrypt_payload(test_payload)

    cached_data = {
        "is_encrypted": True,
        "payload": {"ciphertext": encrypted},
    }

    class _FakeRedis:
        def mget(self, keys):
            return [json.dumps(cached_data)]

    store._redis_client = _FakeRedis()

    result = await store._redis_fetch_rows("chat", ["id-1"])
    assert "id-1" in result
    assert result["id-1"]["text"] == "secret"


# -----------------------------------------------------------------------------
# Tests for Cleanup Worker Edge Cases (lines 1476-1479, 1487)
# -----------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_run_cleanup_once_no_model(pipe_instance):
    """Test _run_cleanup_once returns early when no model (lines 1474-1475)."""
    store = pipe_instance._artifact_store
    store._item_model = None
    await store._run_cleanup_once()


@pytest.mark.asyncio
async def test_run_cleanup_once_executes(pipe_instance):
    """Test _run_cleanup_once executes cleanup (lines 1476-1479)."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store

    cleanup_called = []

    def _fake_cleanup(cutoff):
        cleanup_called.append(cutoff)

    store._cleanup_sync = _fake_cleanup

    await store._run_cleanup_once()
    assert len(cleanup_called) == 1


def test_cleanup_sync_no_session_factory(pipe_instance):
    """Test _cleanup_sync returns early when no session_factory (line 1487)."""
    store = pipe_instance._artifact_store
    store._session_factory = None
    store._item_model = _FakeModel
    # Should not crash
    store._cleanup_sync(datetime.datetime.now(datetime.UTC))


# -----------------------------------------------------------------------------
# Tests for Circuit Breaker Edge Cases (line 1511, 1515)
# -----------------------------------------------------------------------------


def test_db_breaker_allows_empty_user_id(pipe_instance):
    """Test _db_breaker_allows returns True for empty user_id (line 1511)."""
    store = pipe_instance._artifact_store
    assert store._db_breaker_allows("") is True
    assert store._db_breaker_allows(None) is True


def test_db_breaker_allows_clears_old_failures(pipe_instance):
    """Test _db_breaker_allows clears old failures from window (line 1515)."""
    store = pipe_instance._artifact_store
    import time

    # Add old failures
    old_time = time.time() - store._breaker_window_seconds - 10
    store._db_breakers["test-user"].append(old_time)
    store._db_breakers["test-user"].append(old_time)

    # Should return True because old failures are cleared
    assert store._db_breaker_allows("test-user") is True


# -----------------------------------------------------------------------------
# Tests for normalize_persisted_item Edge Cases
# -----------------------------------------------------------------------------


def test_normalize_persisted_item_non_dict():
    """Test normalize_persisted_item returns None for non-dict."""
    assert normalize_persisted_item(None) is None
    assert normalize_persisted_item("string") is None
    assert normalize_persisted_item(123) is None


def test_normalize_persisted_item_no_type():
    """Test normalize_persisted_item returns None for dict without type."""
    assert normalize_persisted_item({}) is None
    assert normalize_persisted_item({"key": "value"}) is None


def test_normalize_persisted_item_function_call_output():
    """Test normalize_persisted_item for function_call_output."""
    item = {"type": "function_call_output", "output": None}
    result = normalize_persisted_item(item)
    assert result is not None
    assert result["output"] == ""
    assert "id" in result
    assert "call_id" in result


def test_normalize_persisted_item_function_call_missing_name():
    """Test normalize_persisted_item for function_call missing name."""
    item = {"type": "function_call", "arguments": "{}"}
    result = normalize_persisted_item(item)
    assert result is None


def test_normalize_persisted_item_function_call_non_string_args():
    """Test normalize_persisted_item for function_call with non-string arguments."""
    item = {"type": "function_call", "name": "test", "arguments": {"key": "value"}}
    result = normalize_persisted_item(item)
    assert result is not None
    assert result["arguments"] == '{"key": "value"}'


def test_normalize_persisted_item_function_call_non_serializable_args():
    """Test normalize_persisted_item for function_call with non-serializable arguments."""
    class _NonSerializable:
        pass

    item = {"type": "function_call", "name": "test", "arguments": _NonSerializable()}
    result = normalize_persisted_item(item)
    assert result is not None
    # Falls back to str()
    assert "NonSerializable" in result["arguments"]


def test_normalize_persisted_item_reasoning_with_string_content():
    """Test normalize_persisted_item for reasoning with string content."""
    item = {"type": "reasoning", "content": "test content"}
    result = normalize_persisted_item(item)
    assert result is not None
    assert result["content"] == [{"type": "reasoning_text", "text": "test content"}]


def test_normalize_persisted_item_reasoning_with_non_list_summary():
    """Test normalize_persisted_item for reasoning with non-list summary."""
    item = {"type": "reasoning", "content": [], "summary": "a summary"}
    result = normalize_persisted_item(item)
    assert result is not None
    assert result["summary"] == ["a summary"]


def test_normalize_persisted_item_file_search_call():
    """Test normalize_persisted_item for file_search_call."""
    item = {"type": "file_search_call", "queries": "not a list"}
    result = normalize_persisted_item(item)
    assert result is not None
    assert result["queries"] == []


def test_normalize_persisted_item_web_search_call():
    """Test normalize_persisted_item for web_search_call."""
    item = {"type": "web_search_call", "action": "not a dict"}
    result = normalize_persisted_item(item)
    assert result is not None
    assert result["action"] == {}


# -----------------------------------------------------------------------------
# Tests for Shutdown
# -----------------------------------------------------------------------------


def test_shutdown_no_executor(pipe_instance):
    """Test shutdown handles no executor gracefully."""
    store = pipe_instance._artifact_store
    store._db_executor = None
    store.shutdown()


def test_shutdown_with_cancel_futures(pipe_instance):
    """Test shutdown with cancel_futures support."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store
    store.shutdown()
    assert store._db_executor is None


def test_shutdown_without_cancel_futures(pipe_instance, monkeypatch):
    """Test shutdown handles TypeError from old ThreadPoolExecutor."""
    _install_fake_store(pipe_instance)
    store = pipe_instance._artifact_store

    class _OldExecutor:
        def shutdown(self, wait=False, cancel_futures=None):
            if cancel_futures is not None:
                raise TypeError("unexpected keyword argument")

    store._db_executor = _OldExecutor()
    store.shutdown()
    assert store._db_executor is None


# -----------------------------------------------------------------------------
# Tests for _wait_for
# -----------------------------------------------------------------------------


@pytest.mark.asyncio
async def test_wait_for_sync_value():
    """Test _wait_for returns sync value immediately."""
    result = await _wait_for(42)
    assert result == 42


@pytest.mark.asyncio
async def test_wait_for_async_without_timeout():
    """Test _wait_for awaits async value without timeout."""
    async def _coro():
        return "async result"

    result = await _wait_for(_coro())
    assert result == "async result"


@pytest.mark.asyncio
async def test_wait_for_async_with_timeout():
    """Test _wait_for awaits async value with timeout."""
    async def _coro():
        return "async result"

    result = await _wait_for(_coro(), timeout=5.0)
    assert result == "async result"


# -----------------------------------------------------------------------------
# Tests for _encode_crockford
# -----------------------------------------------------------------------------


def test_encode_crockford_negative():
    """Test _encode_crockford raises for negative values."""
    with pytest.raises(ValueError):
        _encode_crockford(-1, 4)


def test_encode_crockford_zero():
    """Test _encode_crockford handles zero."""
    result = _encode_crockford(0, 4)
    assert result == "0000"


# -----------------------------------------------------------------------------
# Tests for _sanitize_table_fragment
# -----------------------------------------------------------------------------


def test_sanitize_table_fragment_empty():
    """Test _sanitize_table_fragment handles empty input."""
    result = _sanitize_table_fragment("")
    assert result == "pipe"

    result = _sanitize_table_fragment(None)
    assert result == "pipe"


def test_sanitize_table_fragment_special_chars():
    """Test _sanitize_table_fragment normalizes special characters."""
    result = _sanitize_table_fragment("My-Table.Name!")
    assert "_" in result or result.isalnum()
    assert result == result.lower()
